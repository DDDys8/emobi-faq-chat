import os
import streamlit as st
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI

# âœ… APIã‚­ãƒ¼ã‚’secretsã‹ã‚‰å®‰å…¨ã«èª­ã¿è¾¼ã‚€
openai_key = st.secrets["OPENAI_API_KEY"]

# âœ… LLMï¼ˆGPT-3.5ï¼‰ã‚’åˆæœŸåŒ–
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    openai_api_key=openai_key,
    temperature=0.3
)

# âœ… ãƒ™ã‚¯ãƒˆãƒ«DBã®èª­ã¿è¾¼ã¿ï¼ˆchroma_store ã¯ GitHub ä¸Šã«é…ç½®æ¸ˆï¼‰
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = Chroma(
    persist_directory="./chroma_store",
    embedding_function=embedding_model
)

# âœ… Retrieverã®è¨­å®š
retriever = db.as_retriever(search_kwargs={"k": 3})

# âœ… QAãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ï¼ˆGPT + æ¤œç´¢ï¼‰
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# âœ… Streamlit UI
st.set_page_config(page_title="Emobi AIãƒãƒ£ãƒƒãƒˆ - FINUA", page_icon="ğŸš™")

st.title("ğŸš™ Emobi AIãƒãƒ£ãƒƒãƒˆ - FINUA")
st.write("Emobiã«é–¢ã™ã‚‹ã‚ˆãã‚ã‚‹è³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚")

query = st.text_input("ã”è³ªå•ã‚’ã©ã†ã")

if query:
    result = qa_chain.invoke({"query": query})

    st.subheader("ğŸ“Œ å›ç­”")
    st.write(result["result"])

    st.subheader("ğŸ“š å‚ç…§ã•ã‚ŒãŸFAQ")
    for i, doc in enumerate(result["source_documents"]):
        st.markdown(f"**ãƒãƒ£ãƒ³ã‚¯{i+1}**\n\n```\n{doc.page_content[:500]}\n```")

from langchain_community.embeddings import HuggingFaceEmbeddings

# CPU å¼·åˆ¶æŒ‡å®šï¼ˆStreamlit Cloud ã§ã¯GPUéå¯¾å¿œã®ãŸã‚ï¼‰
embedding_model = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}
)

